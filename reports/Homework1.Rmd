---
title: "Homeworks from chapter 1 to 5"
author: "Joaquin Rodriguez"
date: "03/01/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(xlsx)
library(knitr)
```


### 1.3 Each of 100 multiple-choice questions on an exam has four possible answers but one correct response. For each question, a student randomly selects one response as the answer.

#### a. Specify the distribution of the student’s number of correct answers on the exam.
It is a binomial distribution since the outcome is either a right or wrong answer.  
The distribution has n = 100 and $\pi = 1/4 = 0.25$

#### b.Based on the mean and standard deviation of that distribution, would it be surprising if the student made at least 50 correct responses? Explain your reasoning.
Yes, it would be surprising. The mean is n$\pi = 25$ and the $\sigma$=4.33. Therefore, making at least 50 correct responses means that the student performed many times the $\sigma$ above the mean.


### 1.6 Genotypes AA, Aa, and aa occur with probabilities (π 1 , π 2 , π 3 ). For n = 3 independent observations, the observed frequencies are (n 1 , n 2 , n 3 ).

#### a. Explain how you can determine n 3 from knowing n 1 and n 2 . Thus, the multinomial distribution of (n 1 , n 2 , n 3 ) is actually two-dimensional
I can determine n3 by solving the following equaltion: $n_3=3-(n_1+n_2)$

#### b. Show the set of all possible observations, (n 1 , n 2 , n 3 ) with n = 3.
First I create the grid of all possible combination, then I apply the constrain (the sum for each row should be 3)
```{r}
grid <- expand.grid(0:3,0:3,0:3)
grid[rowSums(grid)==3,]
```


#### c. Suppose (π 1 , π 2 , π 3 ) = (0.25, 0.50, 0.25). Find the multinomial probability that (n 1 , n 2 , n 3 ) = (1, 2, 0).

```{r}
prob <- c(.25, .5, .25)
res <- c(1, 2, 0)
```

The ${\chi}^{2}$ test gives a p-value for the likelihood that the given the probabilities $(.25, .5, .25)$ it generates a contingency table with $(1, 2, 0)$.

```{r}
chisq.test(res,p=prob, correct = T)
```

#### d. Refer to (c). What probability distribution does n 1 alone have? Specify the values of the sample size index and parameter for that distribution.
It has a binomial distribution with the following characteristics: $BIN~(3\frac{1}{4}, 3\frac{1}{4}\frac{3}{4})$


### 1.7 In his autobiography A Sort of Life, British author Graham Greene described a period of severe mental depression during which he played Russian Roulette. This “game” consists of putting a bullet in one of the six chambers of a pistol, spinning the chambers to select one at random, and then ﬁring the pistol once at one’s head.

#### a. Greene played this game six times, and was lucky that none of them resulted in a bullet firing. Find the probability of this outcome.
The probability of playing six times and resulting in the bullet never never firing is: $ (5 / 6) ^ 6 = $`r (5/6)^6`.  

#### b. Suppose one kept playing this game until the bullet ﬁres. Let Y denote the number of the game on which the bullet ﬁres. Argue that the probability of the outcome y equals (5/6) y−1 (1/6), for y = 1, 2, 3, . . . . (This is called the geometric distribution.)
The formula to calculate the probability of k failures before the first success, where the probability of success is p and the probability of failure is q = 1 − p is: $P(Y = y) = q^{y-1} p$. Where y is the number of successes before failure.

### 1.12 To collect data in an introductory statistics course, recently I gave the students a questionnaire. One question asked whether the student was a vegetarian. Of 25 students, 0 answered “yes.” They were not a random sample, but let us use these data to illustrate inference for a proportion. (You may wish to refer to Section 1.4.1 on methods of inference.) Let π denote the population proportion who would say “yes.” Consider H 0 : π = 0.50 and H a : π  = 0.50.

#### a. What happens when you try to conduct the “Wald test,” for which z = √ (p − π 0 )/ [p(1 − p)/n] uses the estimated standard error?
The standard error in this case equals to zero. Therefore, the Wald test statistic equals to $\infty$.

#### b. Find the 95% “Wald conﬁdence interval” (1.3) for π. Is it believable? (When the observation falls at the boundary of the sample space, often Wald methods do not provide sensible answers.)
The CI for $\pi$ is (0, 0). This CI is not believable since we expect that in the whole population there will be at least some vegetetarians. 

#### c. Conduct the “score test,” for which z = (p − π 0 )/ [π 0 (1 − π 0 )/n] uses the null standard error. Report the P-value.
Substituting the value to the formulate we get z = (0 − 0.50)/ 0.50(0.50)/25 = −5.0. Therefore, the p-value < 0.0001.

#### d. Verify that the 95% score conﬁdence interval (i.e., the set of π 0 for which |z| < 1.96 in the score test) equals (0.0, 0.133). (Hint: What do the z test statistic and P-value equal when you test H 0 : π = 0.133 against H a : π  = 0.133.)
The critical value for the $\pi$ value of 0.133 is z = (0 − 0.133)/ 0.133(0.133)/25 = −1.96. Therefore, 0.133 is the null value which corresponds to a a p-value of 0.05.



### 1.13 Refer to the previous exercise, with y = 0 in n = 25 trials.

#### a. Show that l 0 , the maximized likelihood under H 0 , equals (1 − π 0 ) 25 , which is (0.50) 25 for H 0 : π = 0.50.
The binomial probability formula for y = 0 with 25 trials equals to $(1- \pi_0)*25$.

#### b. Show that l 1 , the maximum of the likelihood function over all possible π values, equals 1.0. (Hint: This is the value at the ML estimate value of 0.0.)
The maximum of $(1 − \pi)25$ occurs at $\pi = 0.0$. Therefore, the maximum of the likelihood function l0 equals 1.

#### c. For H 0 : π = 0.50, show that the likelihood-ratio test statistic, −2 log($l_0$ /$l_1$ ), equals 34.7. Report the P-value.
−2 log($l_0$ / $l_1$) = −2 log[(0.50) 25 /1.0] = 34.7. This correspondes to a p-value < 0.0001.

#### d. The 95% likelihood-ratio conﬁdence interval for π is (0.000, 0.074).Verify that 0.074 is the correct upper bound by showing that the likelihood-ratio test of H 0 : π = 0.074 against H a : π  = 0.074 has chi-squared test statistic equal to 3.84 and P -value = 0.05.

$−2 log(l_0 /l_1 ) = −2 log[(0.926)^{25} /1.0] = 3.84$.  
A chi-square with df=1, and value 3.84 assumes a p-value of P -value = 0.05.

### 1.16 Using calculus, it is easier to derive the maximum of the log of the likelihood function, L = log , than the likelihood function  itself. Both functions have maximum at the same value, so it is sufﬁcient to do either.

#### a. Calculate the log likelihood function L(π) for the binomial distribution (1.1).
Log-likelihood function for the binomial distribution is the following: $L(\pi)=ln\frac{n!}{y!(n-y)!}\ y\ln\pi\ (n-y)\ln(1-\pi)$

#### b. One can usually determine the point at which the maximum of a log likelihood L occurs by solving the likelihood equation. This is the equation resulting from differentiating L with respect to the parameter, and setting the derivative equal to zero. Find the likelihood equation for the binomial distribution, and solve it to show that the ML estimate equals p = y/n.

The derivative of the equation in task a with respect to p and set to 0 is:
$\frac{\delta}{\delta p}ln{\frac{n!}{y!(n-y)!}}\ y\ln{p} (n-y)\ln(1-p)$

which is $\frac{y}{p}-\frac{n-y}{1-p}=0\Rightarrow p=\frac{y}{n}$


### 2.2 For diagnostic testing, let X = true status (1 = disease, 2 = no disease) and Y = diagnosis (1 = positive, 2 = negative). Let π i = P (Y = 1|X = i), i = 1, 2.

#### a. Explain why sensitivity = π 1 and speciﬁcity = 1 − π 2.
Sensitivity is the probability of a true positive. Therefore, it means that given the presence of the disease, the test is positive. It can be espressed as $\pi_1=P(Y=1\ |\ X=1)$

Specificity is the probability of a true negative. Therefore, it means that in absence of a disease, the test is negative. It can be espressed as $\pi_2=P(Y=2\ |\ X=2)$ which is equivalent to $1-\pi_2=P(Y=1\ |\ X=2)$

#### b. Let γ denote the probability that a subject has the disease. Given that the diagnosis is positive, use Bayes’s theorem to show that the probability a subject truly has the disease is:


Given: $P(X=1)=\gamma$

$P(Y=1|X=1)=\frac{P(Y=1 \ \cap \ X=1)}{P(X=1)} = \frac{\pi_1\gamma}{\pi_1\gamma\ +\ \pi_2(1-\gamma)}$ 
where the denominator is the overall probability of having a dease, thus the sum of true positive and false negatives.

#### c. For mammograms for detecting breast cancer, suppose γ = 0.01, sensitivity = 0.86, and speciﬁcity = 0.88. Given a positive test result, ﬁnd the probability that the woman truly has breast cancer.

For $\gamma=.01$

Probability that a woman truly has breast cancer is :
```{r}
#probability of true positive p x sensitivity
tpos <- .01*.86
#probability of false negative
fneg <- .99*(1-.88)
#probability that given a positive test the woman truely has breast cancer
tpos/fneg
```

#### d. To better understand the answer in (c), ﬁnd the joint probabilities for the 2 × 2 cross classiﬁcation of X and Y. Discuss their relative sizes in the two cells that refer to a positive test result.

```{r}
df <- matrix(c(.01*.86,0,.99*.12,.99*.88),dimnames = list(c('Ptest', 'Ntest'),c('Pcancer','Ncancer')),nrow=2)
df
df/sum(df)
```
The relative size for a positive test shows that - given a positive test - a person most likely won't actually have breast cancer. (the probability to actually have it is ~7%).

### 2.3 According to recent UN ﬁgures, the annual gun homicide rate is 62.4 per one million residents in the United States and 1.3 per one million residents in the UK.

#### a. Compare the proportion of residents killed annually by guns using the (i) difference of proportions, (ii) relative risk.

The differences in proportions are:
```{r}
US <- 62.4/1000000
UK <- 1.3/1000000

US-UK
```

Where as, the relative risk is:
```{r}
US/UK
```

#### b. When both proportions are very close to 0, as here, which measure is more useful for describing the strength of association? Why?
When both proportion are close to zero, the relative risk is a better representation of their association because it provdes an unbiased (note related to their size) indicator of their relationship.


### 2.7 For adults who sailed on the Titanic on its fateful voyage, the odds ratio between gender (female, male) and survival (yes, no) was 11.4. (For data, see R. Dawson, J. Statist. Educ. 3, no. 3, 1995.)

#### a. What is wrong with the interpretation, “The probability of survival for females was 11.4 times that for males”? Give the correct interpretation.
Given the odss-ratio formula: $\theta=\frac{\pi_1/(1-\pi_1)}{\pi_2/(1-\pi_2)}$  
In this case: $\theta=11.4$  
The correct interpretation expresses how many more times a subject was likely to survive rather than die, given that he was a male.

#### b. The odds of survival for females equaled 2.9. For each gender, ﬁnd the proportion who survived.
```{r}
oddsF <- 2.9
ratio <- 11.4

#proportion of female survived 
oddsF/(1+oddsF)
#proportion of male survived
oddsM <- oddsF*ratio
oddsM/(1+oddsM)
```

#### c. Find the value of R in the interpretation, “The probability of survival for females was R times that for males.”
The probability of survival for females compared to males was:
```{r}
oddsF / (1+oddsF) / oddsM / (1+oddsM)
```


### 2.17 Refer to Table 2.3. Find the P -value for testing that the incidence of heart attacks is independent of aspirin intake using (a) X 2 , (b) G 2 . Interpret results.

#### a.
```{r}
df <- matrix(c(189,104,10845,10933), ncol=2,dimnames = list(c("placebo",'aspirin'), c('yes','no')))
df

chisq.test(df)

#expected frequency estimate
chisq.test(df)$expected
```
The chi-sq is significant, therefore the two distributions are not indipendent.

#### b. 
```{r}
library(RVAideMemoire)
G.test(df)
```
The p-value <0.05, therefore we reject the independet model.

### 2.22 Table 2.15 classiﬁes a sample of psychiatric patients by their diagnosis and by whether their treatment prescribed drugs.

#### a. Conduct a test of independence, and interpret the P -value.
```{r}
df <- matrix(c(105,12,18,47,0,8,2,19,52,13),ncol=2)
df
chisq.test(df)
```
Given that the p-value < 0.05 we reject the test of independence.

#### b. Obtain standardized residuals, and interpret.
```{r}
chisq.test(df)$stdres
```
Chi-square test rejects independency, however the estimation might be inaccurate (as displayed in the R warning message) due to the low number of expected values in some cells.

#### c. Partition chi-squared into three components to describe differences and similarities among the diagnoses, by comparing (i) the ﬁrst two rows, (ii) the third and fourth rows, (iii) the last row to the ﬁrst and second rows combined and the third and fourth rows combined.
```{r}
#first two rows
chisq.test(df[1:2,])
#3rd-4th rows
chisq.test(df[3:4,])
#1st-5h
chisq.test(df[c(1,5),])

#1st+2nd VS 5th
chisq.test(t(matrix(c(df[5,],apply(df[1:2,],1,sum)),nrow =2)))
#3st+4nd VS 5th
chisq.test(t(matrix(c(df[5,],apply(df[3:4,],1,sum)),nrow =2)))
```

### 2.30 Table 2.17 contains results of a study comparing radiation therapy with surgery in treating cancer of the larynx. Use Fisher’s exact test to test H 0 : θ = 1 against H a : θ > 1. Interpret results.

```{r}
matrix(c(21,15,2,3), ncol=2) %>% 
  fisher.test(alternative='greater')
```
The hypothesis of independence is rejected (p-value > 0.38)

### 2.35 At each age level, the death rate is higher in South Carolina than in Maine, but overall the death rate is higher in Maine. Explain how this could be possible. (For data, see H. Wainer, Chance, 12: 44, 1999.)
We can observe from the data how the age distribution is quite different between the two States. In Maine the population, on average, appears to be older. Therefore, given an older population we can expect that the numbers of deaths tend to be higher compared to a younger one.

### 2.39 True, or false?

#### a. In 2 × 2 tables, statistical independence is equivalent to a population odds ratio value of θ = 1.0.
True: odds-ratio of 1 means independence

#### b. We found that a 95% conﬁdence interval for the odds ratio relating having a heart attack (yes, no) to drug (placebo, aspirin) is (1.44, 2.33). If we had formed the table with aspirin in the ﬁrst row (instead of placebo), then the 95% conﬁdence interval would have been (1/2.33, 1/1.44) = (0.43, 0.69).
False: odds ratio are invariant to the orientation of the table

#### c. Using a survey of college students, we study the association between opinion about whether it should be legal to (1) use marijuana, (2) drink alcohol if you are 18 years old. We may get a different value for the odds ratio if we treat opinion about marijuana use as the response variable than if we treat alcohol use as the response variable.
True: odds ratio are invariant to the orientation of the table

#### d. Interchanging two rows or interchanging two columns in a contingency table has no effect on the value of the X 2 or G 2 chi-squared statistics. Thus, these tests treat both the rows and the columns of the contingency table as nominal scale, and if either or both variables are ordinal, the test ignores that information.
True: because it is testing the independence of rows and columns

#### e. Suppose that income (high, low) and gender are conditionally independent, given type of job (secretarial, construction, service, professional, etc.). Then, income and gender are also independent in the 2 × 2 marginal table (i.e., ignoring, rather than controlling, type of job).
False: Not necessarily it depends on the association btw the 2 variables


### 3.2 In the 2000 US Presidential election, Palm Beach County in Florida was the focus of unusual voting patterns apparently caused by a confusing “butterﬂy ballot.” Many voters claimed they voted mistakenly for the Reform party candidate, Pat Buchanan, when they intended to vote for Al Gore. Figure 3.8 shows the total number of votes for Buchanan plotted against the number of votes for the Reform party candidate in 1996 (Ross Perot), by county in Florida. (For details, seeA.Agresti and B. Presnell, Statist. Sci., 17: 436–440, 2003.)

#### a. In county i, let π i denote the proportion of the vote for Buchanan and let x i denote the proportion of the vote for Perot in 1996. For the linear probability model ﬁtted to all counties except Palm Beach County, πˆ i = −0.0003 + 0.0304x i . Give the value of P in the interpretation: The estimated proportion vote for Buchanan in 2000 was roughly P% of that for Perot in 1996.

For each Perot's voters, the proportion of votes received for Buchanan is:
```{r}
-.0003 + .0304 * 1
```

#### b. For Palm Beach County, π i = 0.0079 and x i = 0.0774. Does this result appear to be an outlier? Investigate, by ﬁnding π i /πˆ i and π i − πˆ i . (Analyses conducted by statisticians predicted that fewer than 900 votes were truly intended for Buchanan, compared with the 3407 he received. George W. Bush won the state by 537 votes and, with it, the Electoral College and the election. Other ballot design problems played a role in 110,000 disqualiﬁed “overvote” ballots, in which people mistakenly voted for more than one candidate, with Gore marked on 84,197 ballots and Bush on 37,731.)
$\pi_i/\hat{\pi}_i$
```{r}
.0079/(-.0003 + .0304 * .0774)
```

$\pi_i-\hat{\pi}_i$

```{r}
.0079 - (-.0003 + .0304 * .0774)
(log(.0079) - log(-.0003+.0304*.0774))*2
```

Yes, the Palm Beach results appears to be an outlier.

### 3.8 Refer to the previous exercise for the horseshoe crab data.

#### a. Report the ﬁt for the probit model, with weight predictor. 
```{r, echo=FALSE}
dt <- read.xlsx('../data/Table3_2.xlsx', sheetIndex = 1, header = F)
names(dt) <- c('color', 'spine', 'width', 'satell', 'weight')
# if satell>0 then y=1; if satell=0 then y=0; n=1; 
dt$satell <- ifelse(dt$satell > 0, 1, 0)
# weight = weight/1000; color = color - 1;
dt$weight <- dt$weight/1000
dt$color <- dt$color - 1
# if color=4 then dark=0; if color < 4 then dark=1;
dt$dark <- ifelse(dt$color==4, 0, 1)
```

```{r}
summary(fit <- glm(satell ~ weight, data= dt, family=binomial(link="probit")))
```

### b. Find πˆ at the highest observed weight, 5.20 kg.
```{r}
predict(fit, data.frame(weight=5.2))
#cumulative standard normal probability
predict(fit, data.frame(weight=5.2), type='response')
```

### c. Describe the weight effect by ﬁnding the difference between the πˆ values at the upper and lower quartiles of weight, 2.85 and 2.00 kg.
```{r}
predict(fit, data.frame(weight=c(2.85, 2)))
#cumulative standard normal probability
predict(fit, data.frame(weight=c(2.85, 2)), type='response')
```

As the weight decreases, the probability of having a satellite decreases.

### d. Interpret the parameter estimates using characteristics of the normal cdf that describes the response curve.
In the probit transformation $\mu=\frac{-\alpha}{\beta}$
```{r}
-fit$coefficients[1]/fit$coefficients[2]
```

And $\sigma=\frac{1}{\beta}$
```{r}
1/abs(fit$coefficients[2])
```

### 3.11 An experiment analyzes imperfection rates for two processes used to fabricate silicon wafers for computer chips. For treatment A applied to 10 wafers, the numbers of imperfections are 8, 7, 6, 6, 3, 4, 7, 2, 3, 4. Treatment B applied to 10 other wafers has 9, 9, 8, 14, 8, 13, 11, 5, 7, 6 imperfections. Treat the counts as independent Poisson variates having means μ A and μ B . Consider the model log μ = α + βx, where x = 1 for treatment B and x = 0 for treatment A.

#### a. Show that β = log μ B − log μ A = log(μ B /μ A ) and e β = μ b /μ A .
```{r}
dt <- data.frame(treatment= c(rep(0,10), rep(1,10)), 
                 value= c(8,7,6,6,3,4,7,2,3,4,9,9,8,14,8,13,11,5,7,6) )
summary(fit <- glm(value ~ treatment, data= dt, family=poisson(link="log")))
```

$\beta=log\ \mu_B \ -\ log\ \mu_A$
```{r}
diff(tapply(dt$value, dt$treatment, function(x) log(mean(x))))
```

$log(\mu_B\ / \ \mu_A)$
```{r}
log(tapply(dt$value, dt$treatment, 
           function(x) mean(x))[2] / tapply(dt$value, dt$treatment, function(x) mean(x))[1])
```

$e^\beta\ = \ \mu_b\ / \ \mu_a$
```{r}
exp(fit$coefficients[2]) 

tapply(dt$value, dt$treatment, 
       function(x) mean(x))[2] / tapply(dt$value, dt$treatment, function(x) mean(x))[1]
```

#### b. Fit the model. Report the prediction equation and interpret β.
```{r, echo=FALSE}
fit
#multiplicative effect
exp(fit$coefficients[2]) 
```

The multiplicative effect with a +1 increase in x equals the mean of Y at x multiplied by $e^\beta$ . Thus the mean increases of `r exp(fit$coefficients[2])` units as treatment increases of 1 unit.

#### c. Test H 0 : μ A = μ B by conducting the Wald or likelihood-ratio test of H 0 : β = 0. Interpret.
```{r}
logLik(fit)
```
This means that the models fits significantly better than the null model.

#### d. Construct a 95% conﬁdence interval for μ B /μ A . [Hint: Construct one for β = log(μ B /μ A ) and then exponentiate.]
95% CI for $\mu_B \ /\ \mu_A$
```{r}
exp(confint(fit))
```

###3.13 Access the horseshoe crab data of Table 3.2 at www.stat.uﬂ.edu/∼aa/introcda/appendix.html.

#### a. Using x = weight and Y = number of satellites, ﬁt a Poisson loglinear model. Report the prediction equation.
```{r}
dt <- read.xlsx('../data/Table3_2.xlsx', sheetIndex = 1)
names(dt) <- c('color', 'spine', 'width', 'satell', 'weight')

dt$weight <- dt$weight/1000
dt$color <- dt$color - 1

summary(fit2 <- glm(satell ~ weight, data= dt, family = poisson(link='log')))
```

#### b. Estimate the mean of Y for female crabs of average weight 2.44 kg.
Mean fo Y for weight of 2.44:
```{r}
exp(predict(fit2, data.frame(weight=2.44)))
```

#### c. Use β to describe the weight effect. Construct a 95% conﬁdence interval for β and for the multiplicative effect of a 1 kg increase.
95%-CI for $\beta$ for the multiplicative effect of a 1kg increase.
```{r}
exp(confint(fit2))
```

#### d. Conduct a Wald test of the hypothesis that the mean of Y is independent of weight. Interpret.
The Wald test statistic is $z=\hat\beta\ /\ SE$
```{r, message=FALSE}
coefficients(fit)[2]

#SE
summary(fit2)$coefficients[2,2]

#z^2 has an approximate chi-sq distribution with df=1
(coefficients(fit)[2]/summary(fit2)$coefficients[2,2])^2
dchisq((coefficients(fit)[2]/summary(fit2)$coefficients[2,2])^2,1)
```
The Wald test result is significant, therefore Y and weight are not independent.

#### e. Conduct a likelihood-ratio test about the weight effect. Interpret.
The likelihood-ratio test:
```{r}
#p value for -2(L0 - L1)
dchisq(-2*(logLik( glm(satell ~ 1, data= dt, family = poisson(link='log'))) - logLik(fit2)), 1)
```
The model with weight fits significantly better than the null model (that is an intercept only model)

### 3.18 Table 3.8 lists total attendance (in thousands) and the total number of arrests in a season for soccer teams in the Second Division of the British football league.

#### a. Let Y denote the number of arrests for a team with total attendance t. Explain why the model E(Y) = μt might be plausible. Show that it has alternative form log[E(Y)/t] = α, where α = log(μ), and express this model with an offset term.
```{r}
dt <- data.frame(
  team=letters[1:23],
  attendance= c(404, 286, 443, 169, 222, 150, 321, 189, 258, 223, 211, 215, 108, 210, 224, 211, 168, 185, 158, 429, 226, 150, 148),
  arrests= c(308, 197, 184, 149, 132, 126, 110, 101, 99, 81, 79, 78, 68, 67, 60, 57, 55, 44, 38, 35, 29, 20, 19))
```

```{r, fig.width=5, fig.height=5}
dt %>% 
  ggplot(aes(attendance, arrests)) + 
  geom_point()
```

Because it is plausible that the number of arrests has a direct relationship with attendance.

#### b. Assuming Poisson sampling, ﬁt the model. Report and interpret μ.
$\mu$ is the expected count $E(Y)$ (which has to be exponentiated since we are using log link function)
```{r}
summary(fit3 <- glm(arrests ~ 1 + offset(log(attendance)) , data= dt, family = poisson(link='log')))
```

#### c. Plot arrests against attendance, and overlay the prediction equation. Use residuals to identify teams that had a much larger or smaller than expected number of arrests.

```{r}
dt$predic <- exp(predict(fit3,data.frame(attendance=dt$attendance)))
dd <- gather(data= dt, observation, arrests, arrests, predic)
ggplot(dd, aes(y=attendance, x=arrests, color=observation)) +
  geom_point()
```

Large residulas: 
```{r}
residuals(fit3)[residuals(fit3)>2]
```

#### d. Now ﬁt the model log[E(Y)/t] = α by assuming a negative binomial distribution. Compare αˆ and its SE to what you got in (a). Based on this information and the estimate of the dispersion parameter and its SE, does the Poisson assumption seem appropriate?
```{r}
library(MASS)
summary(glm.nb(arrests ~ 1 + offset(log(attendance)) , data= dt))
```

The dispertion parameter suggests that there is overdispertion in the models.

### 4.1 A study used logistic regression to determine characteristics associated with Y = whether a cancer patient achieved remission (1 = yes). The most important explanatory variable was a labeling index (LI) that measures proliferative activity of cells after a patient receives an injection of tritiated thymidine. It represents the percentage of cells that are “labeled.” Table 4.8 shows the grouped data. Software reports Table 4.9 for a logistic regression model using LI to predict π = P (Y = 1).

#### a. Show how software obtained πˆ = 0.068 when LI = 8.

```{r}
library(xlsx)
dt <- read.xlsx('../data/Table3_2.xlsx', sheetIndex = 1, header = F)
names(dt) <- c('color', 'spine', 'width', 'satell', 'weight')
# if satell>0 then y=1; if satell=0 then y=0; n=1; 
dt$satell <- ifelse(dt$satell > 0, 1, 0)
# weight = weight/1000; color = color - 1;
dt$weight <- dt$weight/1000
dt$color <- dt$color - 1
# if color=4 then dark=0; if color < 4 then dark=1;
dt$dark <- ifelse(dt$color==4, 0, 1)
```

$\hat{\pi}=\frac{e^\eta}{1\ +\ e^\eta}$

```{r}
# calculate pi using the parameters from the fitted model
pi <- function(x) exp(-3.7771 + .1449 * x) / (1 + exp(-3.7771 + .1449 * x))
pi(8)
```

#### b. Show that πˆ = 0.50 when LI = 26.0.

$\hat{\pi}$ = 0.50  
when $-\alpha/\beta = 3.777/0.1449 = 26$

#### c. Show that the rate of change in πˆ is 0.009 when LI = 8 and is 0.036 when LI = 26.

Slope of tangent $\beta\pi(x)[1-\pi(x)]$

```{r}
tang <- function(x) .1449 * pi(x) * (1 - pi(x))
round(tang(8),3)
round(tang(26),3)
```

#### d. The lower quartile and upper quartile for LI are 14 and 28. Show that πˆ increases by 0.42, from 0.15 to 0.57, between those values.

Quartile(14, 28). Show that $\pi$ increases by .42 from .15 to .57 

```{r}
round(abs(pi(14) - pi(28)), 2)
```

#### e. When LI increases by 1, show the estimated odds of remission multiply by 1.16.
Odds of remission for 1 point increase in $e^\beta$:
```{r}
round(exp(.1449), 2)
```

### 4.2 Refer to the previous exercise. Using information from Table 4.9:

#### a. Conduct a Wald test for the LI effect. Interpret.

```{r}
1 - pchisq( (0.1449/0.0593)^2, 1)
```

The value is significant as it is < 0.05. Therefore, the effect of LI is significant.

#### b. Construct a Wald conﬁdence interval for the odds ratio corresponding to a 1-unit increase in LI. Interpret.

$\hat{\beta_i}\pm 1.96\ SE(\hat{\beta_i})$
```{r}
exp(0.1449 + c(-1, 1) * qnorm(.975) * 0.0593)
```

This is the multiplicative effect for a 1 unit increase on odds.
The unexponentiated values indicate the increase of the slope of the effect of LI on y.


#### c. Conduct a likelihood-ratio test for the LI effect. Interpret.

From the textbook table (4.9), the $-2(L_0-L_1)=8.30$ :
```{r}
round(1 - pchisq(8.30, 1), 3)
```
The effect of LI is significant. This is consistent with the conclusion from the Wald-test. 


#### d. Construct the likelihood-ratio conﬁdence interval for the odds ratio. Interpret.


```{r}
c <- c(2,2,3,3,3,1,3,2,1,1,1,1,1,3)
r <- c(0,0,0,0,0,1,2,1,0,1,1,0,1,2)
oddsRatio <- sum(c)/sum(c, r) / (1 - sum(c)/sum(c, r))

oddsRatio + c(-1,1) * qnorm(.975) * sqrt(sum(1/c))
```

The 95% confidence interval includes all the $\beta_0$ values for which the p-value exceeds .05, considering the likelihood-ratio test we are performing: $H_0:\beta=\beta_0$.

### 4.9 For the horseshoe crab data, ﬁt a logistic regression model for the probability of a satellite, using color alone as the predictor.

#### a. Treat color as nominal scale (qualitative). Report the prediction equation, and explain how to interpret the coefﬁcient of the ﬁrst indicator variable.

```{r}
crabs <- read.table("../data/crab.txt",header=T)
names(crabs) <- c("n","color", "spine", "weight", "width", "satellites")

crabs$color <- crabs$color-1
#Looking at the solutions, I think what it does is setting the last category parameter to zero
#and fits in the order 3 - 0 - 1 - 2, while R would pass them in their natural order
crabs$color <- factor(crabs$color, levels = c(3,0,1,2))
```

```{r}
crabs$probSat <- ifelse(crabs$satell>0, TRUE, FALSE)
#to constrain the first category parameter to zero
options(contrasts=c('contr.treatment', 'contr.poly'))
summary(glm(probSat ~ color, data= crabs, family = binomial('logit')))
```

The logit($\pi$)ˆ = −0.76 + 1.86c1 + 1.74c2 + 1.13c3 . The estimated odds a medium-light crab has a satellite are $e^{1.86} = 6.4$ times estimated odds a dark crab has a satellite.

#### b. For the model in (a), conduct a likelihood-ratio test of the hypothesis that color has no effect. Interpret.
```{r}
L0 <- glm(formula = probSat ~ 1, family = binomial("logit"), data = crabs)
L1 <- glm(formula = probSat ~ color, family = binomial("logit"), data = crabs)
anova(L0, L1, test = 'Chisq')
```

Color has an effect, since the model that includes color fits significantly better than the model with the intercept only. The test has 3 degrees of freedom and the difference in the deviance is 13.34 (p-value < 0.05).

#### c. Treating color in a quantitative manner, obtain a prediction equation. Interpret the coefﬁcient of color.

```{r}
L2 <- glm(formula = probSat ~ as.numeric(color), family = binomial("logit"), data = crabs)
summary(L2)
anova(L1, L2, test='Chisq')
```
Treating colors as numeric or ordinal leads to models with a similar fit.


#### d. For the model in (c), test the hypothesis that color has no effect. Interpret.
```{r}
1 - pchisq((-0.7147/0.2095)^2, 1)
```

Color has a significant effect as the above difference is < 0.05.

#### e. When we treat color as quantitative instead of qualitative, state an advantage relating to power and a potential disadvantage relating to model lack of ﬁt.
Using a qualitative variable we can fit a linear trend, and therefore the interpretation of the coefficients continuous instead of discrete. We can then use just one coefficient test of significance to determine whether the variable color is statistically different from zero.

### 4.10 An international poll quoted in anAssociated Press story (December 14, 2004) reported low approval ratings for President George W. Bush among traditional allies of the United States, such as 32% in Canada, 30% in Britain, 19% in Spain, and 17% in Germany. Let Y indicate approval of Bush’s performance (1 = yes, 0 = no), π = P(Y = 1), c 1 = 1 for Canada and 0 otherwise, c 2 = 1 for Britain and 0 otherwise, and c 3 = 1 for Spain and 0 otherwise.

#### a. Explain why these results suggest that for the identity link function, πˆ = 0.17 + 0.15c 1 + 0.13c 2 + 0.02c 3.

When using the identity link we are predicting the straight probabilities. Given that Germany is the baseline, all the other coefficients are the difference compared to the baseline. Therefore, the above values used in the link function make sense.

#### b. Show that the prediction equation for the logit link function is logit(π)ˆ = −1.59 + 0.83c 1 + 0.74c 2 + 0.14c 3.

```{r}
cc <- function(x) log(x/(1-x)) 
#intercept
cc(.17)
#canada
cc(.17) - cc(.32)
#britain
cc(.17) - cc(.30)
#spain
cc(.17) - cc(.19)
```


### 4.19 A sample of subjects were asked their opinion about current laws legalizing abortion (support, oppose). For the explanatory variables gender (female, male), religious afﬁliation (Protestant, Catholic, Jewish), and political party afﬁliation (Democrat, Republican, Independent), the model for the probability π of supporting legalized abortion,

#### a. Interpret how the odds of supporting legalized abortion depend on gender.

```{r}
exp(.16-.0)
```

For females, the odds of supporting abortion are `r exp(.16-.0)` times higher than for males.

#### b. Find the estimated probability of supporting legalized abortion for (i) male Catholic Republicans and (ii) female Jewish Democrats.

For male Catholic Republicans:
```{r}
e <- -0.11 + 0.0 - 0.66 - 1.67
exp(e)/(1+exp(e))
```

For female Jewish Democrats:
```{r}
e <- -0.11 + 0 + 0.16 + 0 + 0.84
exp(e)/(1+exp(e))

```


#### c. If we defined parameters such that the first category of a variable has value 0, then what would $\beta^G_2$  equal? Show then how to obtain the odds ratio that describes the conditional effect of gender.

If $\beta^G_1=0$ then $\beta^G_2=-.16$ 

#### d. If we deﬁned parameters such that they sum to 0 across the categories of a variable, then what would β 1 G and β 2 G equal? Show then how to obtain the odds ratio that describes the conditional effect of gender.

$\beta^G_1=.08$ and $\beta^G_2=-.08$ 

```{r}
exp(.08 - (-.08))
```

### 4.29 Table 4.20 appeared in a national study of 15- and 16-year-old adolescents. The event of interest is ever having sexual intercourse. Analyze these data and summarize in a one-page report, including description and inference about the effects of both gender and race.

Let's first visualize the data in a table.
```{r}
df <- data.frame(race=sort(rep(c('white', 'black'), 4)),
           gender=rep(c('female', 'female', 'male', 'male'), 2),
           intercourse=rep(c('no','yes'),4),
           value=c(36,22,23,29,149,26,134,43))
df

```

Aggregating the data by gender and intercourse (not considering race) we can observe the followin percentages between the groups:
```{r}
df %>% 
  group_by(gender, intercourse) %>% 
  summarise(value=sum(value)) %>% 
  mutate(perc= round(value/sum(value), 2))
```


When controlling for race, we observe that the percentages for intercourse = yes is higher for race black for both males and females.

```{r}
df %>% 
  group_by(race, gender, intercourse) %>% 
  summarise(value=sum(value)) %>% 
  mutate(perc= round(value/sum(value), 2))
```

Since the race variable appers to be a determinant I believe it is worth to examine. The odds ratio for intercourse for black versus white **females** is the following:
```{r}
0.38/(1-0.38) / 0.24/(1-0.24)
```

On the other hand, the ratio for black versus white males is `r 0.56 / (1-0.56) / 0.24 / (1-0.24)` times higher for black versus white males.

```{r}
0.56 / (1-0.56) / 0.24 / (1-0.24)
```
 
 
### 5.1 For the horseshoe crab data (available at www.stat.uﬂ.edu/∼aa/ intro-cda/appendix.html), ﬁt a model using weight and width as predictors.

#### a. Report the prediction equation.

```{r}
crabs$probSat <- ifelse(crabs$satell>0, TRUE, FALSE)
fit <- glm(probSat ~ weight + width, data= crabs, family = binomial('logit'))
summary(fit)
```

$\theta=--9.3 \ +\ .30\ {weight}\ + .82\ {width}$

#### b. Conduct a likelihood-ratio test of H 0 : β 1 = β 2 = 0. Interpret.
```{r}
fit0 <- glm(probSat ~ 1, data= crabs, family = binomial('logit'))
anova(fit0, fit, test = 'Chisq')
```

The $\chi^2$ test indicates that the at least one parameter is significantly different than zero, as the p-value < 0.05.

#### c. Conduct separate likelihood-ratio tests for the partial effects of each variable. Why does neither test show evidence of an effect when the test in (b) shows very strong evidence?
WEIGTH:
```{r}
fitWE <- glm(probSat ~ width , data= crabs, family = binomial('logit'))
summary(fitWE)
anova(fit, fitWE, test='Chisq')
```

WIDTH:
```{r}
fitWI <- glm(probSat ~ weight , data= crabs, family = binomial('logit'))
summary(fitWI)
anova(fit, fitWI, test='Chisq')
```

```{r}
crabs %>% 
  ggplot(aes(weight, width))+
  geom_point()
```

The correlation between width and height is quite high as we can observe from the above graph. Therefore we might have a collinearity issue in the model.

### 5.8 Refer to the classiﬁcation table in Table 5.3 with π 0 = 0.50.


#### a. Explain how this table was constructed.
The table is construncted fitting different values for the sample proportion of 1 outcomes

#### b. Estimate the sensitivity and speciﬁcity, and interpret.

Sensitivity is the probability of a true positive. While, specificity is the probability of a true negative. s

For $\pi_0= .64$ 
```{r}
#sensitivity
74/(74+37)
#specificity
42/(42+20)
```

For $\pi_0= .50$ 
```{r}
#sensitivity
94/(94+17)
#specificity
25/(37+25)
```

### 5.9 Problem 4.1 with Table 4.8 used a labeling index (LI) to predict π = the probability of remission in cancer patients.

#### a. When the data for the 27 subjects are 14 binomial observations (for the 14 distinct levels of LI), the deviance for this model is 15.7 with df = 12. Is it appropriate to use this to check the ﬁt of the model? Why or why not?

```{r}
dt5.8 <- data.frame(LI= c(seq(8, 28, 2),32,34,38), 
           cases=     c(2,2,3,3,3,1,3,2,1,1,1,1,1,3),
           remissions=c(0,0,0,0,0,1,2,1,0,1,1,1,1,3))
kable(dt5.8)
```

It is not reccomended tu use deviance to check the fit in models with continuous predictors.

#### b. The model that also has a quadratic term for LI has deviance = 11.8. Conduct a test comparing the two models.
The likelihood ration statistic for testing the parameter for quadratic term is zero = 3.9.  
This test follows a chi-squared distribution with df = 1. Therefore, the p-value is ~ 0.05.

#### c. Themodelin(b)hasﬁt, logit(π)ˆ = −13.096 + 0.9625(LI ) − 0.0160(LI) 2 , with SE = 0.0095 for β 2 = −0.0160. If you know basic calculus, explain why πˆ is increasing for LI between 0 and 30. Since LI varies between 8 and 38 in this sample, the estimated effect of LI is positive over most of its observed values.

```{r}
df5.9 <- data.frame(
  linear= c(.9652*c(0:30)),
  quadratic = c(-.0160*c(0:30)^2),
  intercept=rep(-13.096,31))
df5.9$eta <- df5.9$linear + df5.9$quadratic + df5.9$intercept

df5.9 %>% tibble::rownames_to_column('LI') %>% mutate(LI=as.numeric(LI)) %>%  
          gather(key = parameter, value = value, linear, quadratic, intercept, eta) %>% 
          ggplot(data=. , aes(x = LI, y = value, color = parameter ) )+
          geom_point()
```

I need to find the critical point where the slope turns negative, namely $\eta'=0\ +\ .96\ -\ 2LI\ .016= 30$

#### d. For the model with only the linear term, the Hosmer–Lemeshow test statistic = 6.6 with df = 6. Interpret.

The Hosmer–Lemeshow test shows good fit at different levels of LI. Therefore, the model is considered adequate.


### 5.20 Refer to Table 2.7 on mother’s drinking and infant malformations.

#### a. Fit the logistic regression model using scores {0, 0.5, 1.5, 4, 7} for alcohol consumption. Check goodness of ﬁt.
Following we visualize the data in tabular form:
```{r, echo=FALSE}
df5.20 <- data.frame(alcohol=c(0,.5, 1.5,4,7), absent=c(17066, 14464,788,126, 37), present=c(48, 38, 5, 1,1))
df5.20$total <- df5.20$absent+df5.20$present
df5.20$percPres <- round(100*df5.20$present/df5.20$total,2)
df5.20$cumulative <- cumsum(df5.20$total)
kable(df5.20)
```

We then fit the model:
```{r}
fit1 <- glm(cbind(present, absent) ~ alcohol, family = binomial(logit),data=df5.20)
summary(fit1)
```

```{r}
fit0 <- glm( cbind(present, absent) ~ 1, family = binomial(logit), data=df5.20 )
anova(fit0, fit1, test = 'Chisq')
```

Goodness of fit of the null model indicates that the full model fits better than the model with itercept only.

#### b. Test independence using the likelihood-ratio test for the model in (a). (The trend test of Section 2.5.1 is the score test for this model.)

Likelihood ratio test for model a(1) is the following:
```{r}
anova(fit1, test = 'Chisq')
```

#### c. The sample proportion of malformations is much higher in the highest alcohol category because, although it has only one malformation, its sample size is only 38. Are the results sensitive to this single observation? Re-ﬁt the model without it, entering 0 malformations for 37 observations, and comparetheresultsofthelikelihood-ratiotest. (Becauseresultsaresensitive to a single observation, it is hazardous to make conclusions, even though n was extremely large.)

```{r}
df5.20[5, c('absent', 'present', 'total')] <- c(37, 0, 37)
df5.20[,1:3] %>% kable()
summary(glm(cbind(absent, present) ~ alcohol, family = binomial(logit), data=df5.20))
```

The new model has a negative coefficient for alcohol where before it has positive. Furthermore, the coefficient from the new model is not significant for alcohol, where before it was.

#### d. Fit the model and conduct the test of independence for all the data using scores {1, 2, 3, 4, 5}. Compare the results with (b). (Results for highly unbalanced data can be sensitive to the choice of scores.)

```{r}
df5.20 <- data.frame(alcohol=c(1:5), absent=c(17066, 14464,788,126, 37), present=c(48, 38, 5, 1,1))
summary(glm(cbind(absent, present) ~ alcohol, family = binomial(logit), data=df5.20))
```

Again, as in task c, sign of the alcohol coefficient is now negative and is also non significant.

### 5.27 About how large a sample is needed to test the hypothesis of equal probabilities sothatP(typeIIerror)=0.05whenπ 1 = 0.40andπ 2 = 0.60, ifthehypothesis is rejected when the P -value is less than 0.01?

It appers that for both $\pi$ cases we need the same sample size, which is n equal to:
```{r}
library(Hmisc)
bsamsize(.4,.6, fraction = .5, alpha = .01, power = .95)
```
